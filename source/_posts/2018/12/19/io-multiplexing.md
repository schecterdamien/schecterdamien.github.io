---
title: io多路复用
date: 2018-12-19 01:15:33
tags: linux
---
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然之前也看过不少关于io多路复用的文章，但是一直一知半解，直到今天看到两篇文章，豁然开朗。没自信写的比这两篇文章更好，只能挑出重点，提出一点自己的看法，当作笔记而已。   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先，打开传送门：  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. <a>https://www.jianshu.com/p/486b0965c296</a>  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. <a>https://www.jianshu.com/p/dfd940e7fca2</a>

### linux五种io模型：###
1. 要点：
   1. io等待其实分为两个阶段，一个是等待数据，一个是把数据从内核复制到用户空间。
   2. select、poll、epoll、recvfrom等都是系统调用，从用户态切换到系统态，有高昂的性能损耗
   3. 进程的阻塞是进程自身的一种主动行为，是为了让出cpu，执行其他任务，不断轮询在系统层面看来本身是一种浪费资源、效率很低的行为，这么做的原因也是因为系统底层没有提供相应的功能，使得上层应用只能采取这种办法。
   4. 这里同步异步是指的这两个阶段有没有联系，如果第一个阶段之后，进程还要等待第二个阶段，那么其实就是同步，异步就是这两个阶段完全没有关系，第一个阶段调用之后，进程就可以不用管了。然后等数据准备好了系统通过回调或信号的方式通知进程，这个过程可以发生在第一阶段之后（信号驱动式IO），也可以发生在第二阶段之后（异步非阻塞 IO）
2. 比较常见的类型：
   1. 同步阻塞io
      1. 最常见最简单的类型，linux下默认所有的socket都是阻塞的，io调用的时候两个阶段进程都在等待
      2. 这种模式想要并发就需要多线程，但是并发不可能太大，因为linux默认一个线程是8m的内存空间，所以10000的并发就是80G（在长链接下很可能达到），系统早就爆炸了！被人利用进行ddos，开启一个链接什么都不做都能把服务器撑爆
   2. 同步非阻塞
      1. socket设置为non-blocking模式，这种情况下，socket调用不会阻塞，会直接返回，然后可以不断调用recvfrom来判断是否有数据来，也可以在轮询之间做点别的事，仔细想想这种代码写出来该有多难看。
      2. 但是，这为同时监控多个socket提供了可能，如果我想提高并发又不想开太多线程可以怎么做呢，可以设置几个队列，read_socket队列，write_socket队列等，每次来请求了，都打开一个socket，然后socket不阻塞，直接返回，放入相应的队列里面，再遍历各个队列，看是否有数据过来，过来就取数据，然后执行相关业务。这种做法有什么缺陷呢：
         1. 业务代码和底层代码强耦合，重构成本高
         2. 这个轮询的事情不该用户进程来做，每次轮询都会使用recvform系统调用，系统调用的代价很高的，而且还是放在千百次的轮询中，大大的消耗资源
         3. ……
      3. 这种模式实现上可以使用两个线程，一个线程监听socket请求，并放入相关队列，一个线程执行轮询，处理业务。
   3. io多路复用
      1. 上面说了，轮询不该用户进程来做，本身是系统层面没提供相关功能之下的一种妥协，于是就有了select、poll、epoll，这几个与用户进程之间的关系差不多，以select为例，
      2. 这种模式socket也需要设置non-blocking，不然还是会阻塞，有请求过来就把socket扔给select，select调用的时候也会阻塞用户进程，但是多个socket，一旦有到达的，select就会通知用户进程，然后用户进程就可以处理。如果同时来了n个请求，用户进程可以同时把n个请求放进数组让select处理，假设业务处理速度很快，可以瞬间完成，那这n个请求的完成时间则取决于io最慢的那个请求的时间，并发就上来了。
      3. 其实本质上和第二种说的实现差不多，就是要有个角色专门来轮询socket，但是好处是业务代码和底层代码接耦了，然后又不会进行千百次的系统调用，因为遍历在内核中完成了。这种模式的io调用第一步的时间被压缩了，但是第二步，从内核拷贝数据到用户态还是会阻塞
      4. 这种情况，需要进行两部系统调用，一个是select，一个是recvform，所以并发不高的情况，select不一定比同步阻塞加多进程性能好，优势在这种情况下能处理更多的连接
   4. 信号驱动式IO
      1. 这种模式需要socket支持，socket调用的时候安装一个信号处理函数，然后进程就继续运行，数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用recvform处理数据，这个时候recvform还是阻塞的。因为进程在调用之后不用管后面的数据处理部分，这部分交给信号处理函数来做，所以是异步的。
   5. 异步非阻塞 IO
      1. 进程进行aio_read系统调用之后，会直接返回。然后等数据准备好之后，内核又把数据复制到进场，然后通知进程，在linux中通知的方式是信号。这个时候，io等待的两步都是非阻塞的，目前开源的异步io库有libevent、libev、libuv.  


### 比较select、poll和epoll ###

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;epoll是Linux所特有，在freeBSD中可以使用kqueue。我们说select低效的原因是因为需要遍历fd，在大量连接下比较慢，线性增长，不如epoll的监听回调，此时epoll还是常数级别响应时间（使用哈希表）。但是监听回调本身就有一定的成本，在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好。

1. select
    1. select是POSIX所规定，一般操作系统均有实现，
    2. 优点：良好的跨平台性
    3. 缺点：select最大的缺陷就是单个进程所打开的FD是有一定限制的，它由FD_SETSIZE设置，默认值是1024，可以更改，但是会降低效率。同时select是遍历socket。需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大
2. poll
    1. poll本质上和select没有区别，也是遍历fd。使用“水平触发”，就是如果报告了fd，进程没有处理，下次poll时还会再次报告这个fd
    2. 优点：没有最大连接数的限制，原因是它是基于链表来存储的
    3. 缺点：大量的fd在用户态和内核态传递，
3. epoll
    1. 是select和poll的增强版本，epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次，还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。
    2. 支持水平触发和边缘触发，水平触发是默认模式。边缘触发就是它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。边缘触发是高速工作方式，只支持no-block socket
    1. 优点：
        1. 没有最大并发连接的限制
        2. 去掉了遍历文件描述符，而是通过监听回调的的机制，所以效率不会随着fd的数目增长而下降，只有活跃的fd才会调用callback
        3. 利用mmap()文件映射内存加速与内核空间的消息传递