<!DOCTYPE html>
<html lang="ch">
  <head>
    <meta charset="utf-8" />
<title></title>
<meta name="description" 
      content=""
>

  




<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="alternate"
  type="application/rss+xml"
  href="https://schecterdamien.github.io/index.xml"
  title=""
/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content=""/>



<link rel="stylesheet" href="https://schecterdamien.github.io/fontawesome/css/all.min.css" />




<link
  crossorigin="anonymous"
  href="/css/styles.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>




<link
  id="dark-mode-theme"
  crossorigin="anonymous"
  href="/css/dark.min.css"
  integrity=""
  rel="preload stylesheet"
  as="style"
/>


<script>
  var darkTheme = document.getElementById('dark-mode-theme')
  var storedTheme = localStorage.getItem('dark-mode-storage')

  if (storedTheme === 'dark') {
    darkTheme.disabled = false
  } else if (storedTheme === 'light') {
    darkTheme.disabled = true
  }
</script>


<script defer crossorigin="anonymous" src="/js/theme.js" integrity=""></script>


<script defer crossorigin="anonymous" src="/js/instantpage.min.js" integrity=""></script><meta name="generator" content="Hugo 0.101.0" />
  </head>
  <body>
    
  
    
  
  



  <div class="intro-header"></div>


    
  <div class="container" role="main">
    <div class="posts-list">
      
        
        

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/http2-wireshark/">
    <h2 class="post-title">HTTP/2 抓包遇到的坑</h2>
  </a>
  <div class="post-entry">
    
      <p>上一篇文章介绍了http2协议相关的细节。因为是为了准备分享，所以为了尽可能直观的展示协议通信（特别是握手协商）的整个过程，还是决定准备一下小的demo然后抓包演示下。在这个过程也遇到了一些问题，耽搁了不少时间，所以记录一下整个过程
h2c抓包 分别对HTTP2的两种建立链接的方式进行抓包（h2c和h2），先演示h2c的连接建立过程，准备的server端的代码如下：
package main import ( &#34;fmt&#34; &#34;log&#34; &#34;net/http&#34; &#34;golang.org/x/net/http2&#34; &#34;golang.org/x/net/http2/h2c&#34; ) // http2 h2c func main() { h2s := &amp;http2.Server{} handler := http.NewServeMux() handler.HandleFunc(&#34;/ping&#34;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprint(w, &#34;pong&#34;) }) s := &amp;http.Server{ Addr: &#34;:5005&#34;, Handler: h2c.NewHandler(handler, h2s), } http2.ConfigureServer(s, &amp;http2.Server{}) log.Fatal(s.ListenAndServe()) } 然后打开wireshark，监听本地的5005端口，使用curl请求server，这里需要指定&ndash;http2使用http2协议，不然的话curl默认使用http1.1：
curl --http2 http://localhost:5005/ping 可以在wireshark里看到建立连接的过程：
上图是客户端(curl)发送的http1.1的协议升级请求。这里能看到上篇文章提到的协议升级的关键的header。Connection指定header有哪些逐跳头部，Upgrade指定客户端希望升级到http2协议，HTTP2-Settings则指定了连接的初始参数。
上图是服务端的http response。返回了Upgrade表示服务端同意升级到http2协议，然后客户端和服务端就可以使用http2通信了。
h2c抓包遇到的问题 虽然在上文已经提到了结论：
go在1.6开始支持HTTP/2，这个时候只支持HTTP/2 over TLS，也就是h2。只要是TLS部署，则http库就会默认进行HTTPS/2协商，协商失败则蜕化为HTTPS/1 go在1.8开始支持 HTTP/2 server push go在1.11开始支持 HTTP/2 h2c gin目前只支持h2方式的HTTP/2，不支持h2c方式的HTTP/2 但是在一开始对于go里是怎么支持http2是不太清楚的，是在第三方库里支持还是net/http直接支持？是h2c和h2两种方式都支持还是只支持h2？google的时候很多文章又语焉不详，所以在准备一段h2c server代码的时候比较曲折。</p>
      <a href="https://schecterdamien.github.io/posts/2019/http2-wireshark/" class="post-read-more"
        ></a
      >
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Dec 23, 2021
  
</span>

  </div>
</article>

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/http2/">
    <h2 class="post-title">HTTP/2 协议</h2>
  </a>
  <div class="post-entry">
    
      <p>用grpc很久了，但是一直都仅限于使用，对于比较深层次的细节还是缺乏了解，想找个时间好好学习下这个黑盒底下的一系列技术，最近刚好要准备一个技术分享，所以就决定分享HTTP/2协议相关的细节，因为grpc的通信就是用的HTTP/2协议，这篇文章算是对准备的HTTP/2分享的一个记录
http应该大家都不陌生，目前使用最多的应该还是HTTP/1.1版本的http协议，那么HTTP/1.1到底有些什么样的问题，导致需要HTTP/2呢？主要有两点
头阻塞：HTTP/1.1中只有收到当前请求响应后才能重用当前tcp连接发送下一次请求。虽然HTTP/1.1提出了pipeline，旨在缓解这个问题。但是一方面pipeline在复杂的网络环境下很难实现和普及，其次就算都用上了pipeline，pipeline也只是使得可以在本次响应完成之前可以发送下次请求，但响应依然要严格按照顺序返回，也就是如果前一个响应被阻塞，后边的响应将不会到来，头阻塞问题还是没有完全解决。目前客户端（比如浏览器）一般会同时打开多个tcp连接来绕开头阻塞的问题，chrome默认会打开6个tcp链接，并发请求各种类型数据。但是这就带来了一个矛盾，tcp链接越多，肯定对资源的浪费是越大的，链接越少，头阻塞的问题就会越突出 重复的未压缩头数据的传输：自HTTP/1.1之后，HTTP请求中通常带有大量ASCII编码的头部，这些头部通常大部分不会变化，需要每次请求都携带（尤其像是User Agent、Cookie这些值比较长的头部字段），会给本来就拥挤的网络带来很大的压力 除此之外还有一些问题，比如TCP 的慢启动，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度，而大量http的业务请求其实本身都只是短链接的，所以导致tcp的流控算法没有很好的应用。为此，HTTP/2的出现就很有必要了
HTTP/2 概述 先说下HTTP/2的发展历程:
2009年，Google提出了一项实验性的协议SPDY（读音同speedy），旨在开发者不修改当前网站实现的前提下，提高页面加载速度。SPDY提出后，Chrome、Firefox、Opera等主流浏览器先后给出了实现，很多大型网站（如Google、Twitter、Facebook等）分别提供了其对SPDY会话的实现 2012年，HTTP-WG提出了在SPDY基础上构建HTTP/2的草案 2013年给出了第一个对HTTP/2的实现，自此HTTP/2、SPDY并行发展，在客户端和服务器上进行了广泛可靠的测试 2015年，Google 宣布放弃对SPDY的继续支持，标志着HTTP/2正式登上历史舞台 那么HTTP/2有些什么特点呢：
HTTP/2 可以让我们的应用更快、更简单、更稳定 通过有效压缩 HTTP 标头字段将协议开销降至最低，同时增加对请求优先级和服务器推送的支持 带来了大量其他协议层面的辅助实现，例如新的流控制、错误处理和升级机制 HTTP/2 没有改动 HTTP 的应用语义，而是修改了数据的格式和传输方式，只需要升级基础设施（代理、web框架等），上层的业务代码都可以不必修改而在新协议下运行 上一个HTTP版本是1.1，为什么这之后不是 HTTP/1.2呢？因为 HTTP/2 引入了一个新的二进制分帧层，该层无法与之前的 HTTP/1.x 服务器和客户端兼容，因此协议的主版本提升到 HTTP/2，且HTTP/2后HTTP协议没有小版本号了，后面直接就是HTTP3
那HTTP/2是怎么做到加入了这么多功能和优化，但是上层业务不需要改代码呢？一切都是因为引入了一个二进制分帧层，并且HTTP/2的新特性也都是建立在这个基础之上的。这又印证了那句话，“计算机中任何问题都可以通过增加一个中间层来解决“。
如上图，HTTP/2.0在应用层（HTTP）和传输层（TCP或者TLS）之间增加一个二进制分帧层，HTTP/2.0通信都在一个TCP连接上完成，这个连接可以承载任意数量的双向数据流，相应的每个数据流以消息的形式发送。而消息由一或多个帧组成，这些帧可以乱序发送，然后根据每个帧首部的流标识符重新组装
这是另一张描述连接（connection）、流（stream）、消息（message）、帧（frame）关系的图，这里解释下这几个概念。
帧（Frame）： 帧是HTTP/2通信的最小单位 请求和响应都分为首部帧和消息帧单独传输，初此之外还有一些其他的类型的帧，下面会介绍帧结构的时候会说 消息（Message）： 指逻辑上的HTTP消息，比如一次请求、一次响应等。 由一个或多个帧组成，以二进制压缩格式存放 HTTP/1 中的头部。 流（Stream）： 是一条逻辑上的连接，是TCP连接中的一个虚拟通道（每个Tcp连接会建立多个steam） 可以承载双向的消息，每个流都有一个唯一的整数标识符，帧会记录Stream 的id 不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ），因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息。同一 Stream 内部的帧必须是严格有序的。 客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。 同一个连接中的 Stream ID 是不能复用的，只能顺序递增，所以当 Stream ID 耗尽时，需要发一个控制帧 GOAWAY，用来关闭 TCP 连接 连接（Connection）： 即TCP连接 这种在一个TCP连接中划分多个逻辑链接，把所有两端的流量都集中在一个TCP连接的做法，减少了服务端的压力，虽然流量还是这么多流量，但是创建的socket会明显减少，内存占用更少，每个连接吞吐量更大。并且HTTP 性能优化的关键并不在于高带宽，而是低延迟，这种做法避免了TCP的慢启动，能更有效的利用到TCP的流控算法。</p>
      <a href="https://schecterdamien.github.io/posts/2019/http2/" class="post-read-more"
        ></a
      >
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Jul 25, 2021
  
</span>

  </div>
</article>

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/mysql-lock/">
    <h2 class="post-title">mysql锁总结</h2>
  </a>
  <div class="post-entry">
    
      <p>最近做了一次和MySQL（innoDB）锁有关的技术分享，记录一下
为了能尽量准确简洁的描述innoDB的锁机制，看了好多的文章，由于innoDB中的锁系统确实非常复杂，细节特别多，如有纰漏和谬误，还请联系改正
innoDB锁简介 innoDb支持多种粒度的锁，按照粒度来分，可分为表锁（LOCK_TABLE）和行锁（LOCK_REC） 一般的锁系统都会有共享锁和排他锁的分类，共享锁也叫读锁，排他锁也叫写锁。加在同一个资源上，写锁会阻塞另外一把写锁或读锁的获取，读锁则允许另外一把读锁的获取，也就是读读之间允许并发，读写或者写写会阻塞，innodb中表锁和行锁都支持共享锁（简写S）和排他锁（简写X）。
因为innoDB支持多粒度的锁，允许表锁和行锁的并存，为了方便多粒度锁冲突的判断，innoDB中还存在一种名叫意向锁（Intention Locks）的锁。
除此之外，还有一种特殊的表锁，自增锁，主要用来并发安全的生成自增id，一种特殊的意向锁，插入意向锁，用来防止幻读问题
表锁 表锁，锁定的粒度是整个表，也分共享锁和排他锁。不同于行锁，表锁MySQL Server层就有实现（所以MyISAM支持表锁，也只支持表锁），innoDb则在存储引擎层面也实现了一遍表锁（后面会介绍具体结构）。
哪些时候会触发表锁呢？在执行某些ddl时，比如alter table等操作，会对整个表加锁，也可以手动执行锁表语句：LOCK TALBES table_name [READ | WRITE]，READ为共享锁，WRITE为排他锁，手动解锁的语句为：UNLOCK TABLES，会直接释放当前会话持有的所有表锁
有一些需要注意的地方：
因为MySQL Server层和InnoDB都实现了表锁，仅当autocommit=0、innodb_table_lock=1（默认设置）时，InnoDB层才能知道MySQL加的表锁，ＭySQL Server才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁，否则，InnoDB将无法自动检测并处理这种死锁 在用LOCK TALBES显式获取锁后，只能访问加锁的这些表，并且如果加的是共享锁，那么只能执行查询操作，而不能执行更新操作，如果获得的是排他锁，则可以进行更新操作。 开始事务时会自动UNLOCK之前的表锁，COMMIT或ROLLBACK都不能释放用LOCK TABLES加的表级锁。LOCK TALBES时会先隐式提交事务，再锁表，UNLOCK TALBES也会隐式提交事务。所以，事务中需要的表锁必须在事务开头一次性获取，无法再事务中间获取，因为不管是LOCK TALBES还是UNLOCK TALBES都会提交事务 官网上建议的表锁的使用方法：
SET autocommit=0; LOCK TABLES t1 WRITE, t2 READ, ...; ... do something with tables t1 and t2 here ... COMMIT; UNLOCK TABLES; 实际业务中，没有特殊理由，不建议使用表锁，因为锁的粒度太大了，极大的影响并发
意向锁 意向锁是一种特殊的表级锁，意向锁是为了让InnoDB多粒度的锁能共存而设计的。取得行的共享锁和排他锁之前需要先取得表的意向共享锁（IS）和意向排他锁（IX），意向共享锁和意向排他锁都是系统自动添加和自动释放的，整个过程无需人工干预。 主要是用来辅助表级和行级锁的冲突判断，因为Innodb支持行级锁，如果没有意向锁，则判断表锁和行锁冲突的时候需要遍历表上所有行锁，有了意向锁，则只要判断表是否存在意向锁就可以知道是否有行锁了。表级别锁的兼容性如下表：
X IX S IS X Conflict Conflict Conflict Conflict IX Conflict Compatible Conflict Compatible S Conflict Conflict Compatible Compatible IS Conflict Compatible Compatible Compatible 可以看到，意向锁与意向锁兼容，IX、IS自身以及相互都兼容，不互斥，因为意向锁仅表示下一层级加什么类型的锁，不代表当前层加什么类型的锁；IX和表级X、S互斥；IS和表级X锁互斥。其兼容性正好体现了其作用</p>
      <a href="https://schecterdamien.github.io/posts/2019/mysql-lock/" class="post-read-more"
        ></a
      >
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Mar 4, 2021
  
</span>

  </div>
</article>

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/pg-mvcc/">
    <h2 class="post-title">PostgreSQL的事务隔离和MVCC</h2>
  </a>
  <div class="post-entry">
    
      <p>因为之前遇到的并发问题，所以又把pg的事务隔离和mvcc实现温习了一遍，稍微整理，遂有此文。
数据库并发问题 说到事务隔离，得先说说数据库可能产生的几种并发问题：
脏读（Dirty Read）：一个事务a读到了事务b更新未提交的数据，b回滚了，a读到的数据就是脏数据 不可重复读（NonRepeatable Read）：事务a多次读取同一个数据，事务b在a多次读取的过程中，对这条数据做出了更改并提交，导致事务a在多次读取结果不一致 幻读（Phantom Read）：事务a在批量更新一个表，事务b这个时候插入了一条不同的记录，导致事务a在改完后发现还有一条记录没有改，就像幻觉一样 不可重复读和幻读很容易混淆，其实幻读是不可重复读的一种特殊情况，只不过不可重复读侧重于修改，幻读侧重于新增或删除，解决不可重复读的问题只需要锁住满足条件的行，解决幻读需要提高事务的隔离级别，但与此同时，事务的隔离级别越高，并发能力也就越低。所以，所以还需要权衡。
事务隔离级别 为了有效保证并发读取数据的正确性，提出的事务隔离级别：
未提交读（Read Uncommitted）：允许脏读，也就是可能读取到其他事务中未提交的修改 提交读(Read Committed)：只能读取到其他事务已经提交的数据。PostgreSQL、Oracle等多数数据库默认都是该级别 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 在SQL标准中，定义了不同隔离级别会出现的并发问题：
隔离级别 脏读 不可重复读 幻读 未提交读 可能 可能 可能 已提交读 不可能 可能 可能 可重复读 不可能 不可能 可能 串行读 不可能 不可能 不可能 注意，这是ANSI SQL标准中的定义，但是具体到数据库的实现可能不一样，只会更严格。比如Read Uncommitted这种隔离级别，其实在实际使用中是没有意义的，设置这种级别还不如用Nosql，所以在pg 中其实Read Uncommitted和Read Committed是一样的。
关于Repeated Read，因为在同一个事务内查询到的数据都是在开启事务后第一次查询时候的快照，可知在业务层开启事务后，事务内所有的乐观锁机制都将失效，毕竟都读不到其他事务的提交了，一定会造成写冲突。除此之外，看了不少的讲pg的事务隔离的文章，都是说pg的Repeated Read不会出现幻读&hellip;然后实际上，自测发现并不能完全避免。因为幻读其实分为好多种，在《A Critique of ANSI SQL Isolation Levels》论文就定义了好多种的幻读。可以说，pg的Repeated Read基于其MVCC的实现可以避免一部分，但是是无法完全避免的。 关于Serializable，pg在9.1之前，是没有Repeated Read这个隔离级别，只有Serializable。9.1之后，才把之前的Serializable重命名为Repeated Read，然后加上一个更为严格的Serializable隔离级别。在这两个隔离级别下，如果两个不同事务中同时修改一条记录都会导致其中某个事物写失败，所以在应用层需要有重试机制。 关于两者的不同，应该是说Serializable这种隔离级别可以解决更多的幻读问题，Serializable使用谓词锁（Predicate Lock）来防止这些幻读，意思是如果一个事务T1正在执行一个查询，该查询的的WHERE子句存在一个条件表达式E1，同时这个查询下面还有其他更新或者插入操作，那么另外一个事务T2 就不能插入或删除任何满足E1的数据行。比如之前遇到的并发问题，其实可以简单表达为(伪代码)：
if not Binding.objects.filter(sku_code=&#39;test&#39;).all(): # 这里只有用all()才能触发pg的谓词锁，first()，exist()都不行 Binding.objects.create(sku_code=&#39;test&#39;) 如果是在Repeated Read下，可能就重复创建两条sku_code=&lsquo;test&rsquo;的Binding了，但是在Serializable里面因为存在读写依赖，并发两条事务中肯定会又一条会失败，会提示&rsquo;could not serialize access due to read/write dependencies among transactions&rsquo;错误</p>
      <a href="https://schecterdamien.github.io/posts/2019/pg-mvcc/" class="post-read-more"
        ></a
      >
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 25, 2019
  
</span>

  </div>
</article>

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/concurrency-problem/">
    <h2 class="post-title">记一次并发问题的排查</h2>
  </a>
  <div class="post-entry">
    
      <p>起源 事情的起源是这样的，Binding是一张多对多的表，主要有sku_code，settlement_sn，enable这几个字段，但是逻辑上sku_code，settlemen_sn，enable=True的在表中应该是唯一的。因为enable=False可能有多条，所以不能在数据库上加联合唯一的索引，但是代码里面有判断。然后有一天测试发现出现两条记录的sku_code，settlemen_sn相同且enable都为True，然后看代码，创建Binding的代码大致是这样的
def create_binding(sku_code, settlement_sn)： if not Binding.objects.filter(sku_code=sku_code, settlement_sn=settlement_sn, enable=True).exist(): raise OperationError(&#39;binding_created&#39;) # 一系列有些耗时的校验 do_some_check() return Binding.objects.create(settlement_sn=settlement_sn, sku_code=sku_code, enable=True) 先判断这样的Binding是否存在，存在的话就直接抛异常，不存在的话再进行其他的校验，最后生成对应的Binding。一般情况下没问题，但是如果并发请求的sku_code和settlement_sn都相同，接到请求的几个worker同时从数据库查询发现目标binding不存在，做出可以创建的判断（因为下面存在一些耗时的校验让这种情况的概率变得很高），然后就创建了sku_code和settlement_sn都相同的记录。 因为是公司内部使用的财务规则系统，也就几个人在用，所以做的时候没怎么考虑并发问题。最后发现其实是前端没有做按钮的防重点，用户连续在按钮上点了两下，造成了这个问题。 我们用的是pg（等下会说为什么强调是pg），这里其实是发生了幻读。那改事务隔离级别可以解决吗，可重复读和串行化不是能解决幻读吗。。。。。肯定不行，一言不合就改数据库事务隔离级别肯定是不行的，而且改了也没法解决这种幻读问题(后来才知道其实串行化可以，但需要加上失败重试的逻辑)。那加锁吧！当时也没多想，迷迷糊糊的在判断上加个锁：
def create_binding(sku_code, settlement_sn)： if not Binding.objects.filter(sku_code=sku_code, settlement_sn=settlement_sn, enable=True).select_for_update).exist(): raise OperationError(&#39;binding_created&#39;) ..... 查询上加上select_for_update()，阻塞住并发的读，完美！发到测试环境，一看，并不行&hellip;还是会生成重，再回头看才明白这个锁有问题。首先，django的select_for_update加上exist()其实是不会生成&rsquo;select &hellip; for update&rsquo;语句的，然后就算生成了，但是因为没有这样的数据，所以没有row可以被锁，这个锁其实没有任何作用。
解决 那怎么解决呢，分布式锁肯定是可以的，给判断的代码块加上锁，这样创建binding的请求就变成串行的了，如果对并发没要求，比如我们这种场景为了防止重点，可以使用这种办法。代码块就变成这样了：
def create_binding(sku_code, settlement_sn)： # 获取分布式锁 get_lock() if not Binding.objects.filter(sku_code=sku_code, settlement_sn=settlement_sn, enable=True).exist(): raise OperationError(&#39;binding_created&#39;) # 一系列有些耗时的校验 do_some_check() result = Binding.objects.create(settlement_sn=settlement_sn, sku_code=sku_code, enable=True) # 释放分布式锁 release_lock() return result 除此之外有没有别的办法呢，其实我们就是想锁住一个“不存在”的一行，google了一下，搜到一篇文章：https://rosscoded.com/blog/2018/05/02/locking-phantom-postgresql/，这里面在pg的数据库级别给出了解决办法。首先可以使用ON CONFLICT语句，其次还可以使用advisory locks，劝告锁，这是pg独有的一种锁，思路挺有意思，不过这是库级别的锁，用多了也不好。但是这些都得在代码里面写sql，最后我们还是选择了基于redis的简单的分布式锁来解决。
回到之前说的，为啥强调是pg呢，因为后来试了一下，在mysql里面，其实是可以给不存在的数据加锁的，mysql里面这个叫gap lock，间隙锁，专门用来解决幻读问题。比如直接select * from binding where sku_code = &quot;123&quot; for update其实会同时加record Lock和gap lock，把已经存在的sku_code=123的行给锁住，同时还会锁住不存在的这些目标行，就是这时其他事务其实是没法写sku_code=123的数据的。mysql只有在可重复读以上的隔离级别才会自动加gap lock。</p>
      <a href="https://schecterdamien.github.io/posts/2019/concurrency-problem/" class="post-read-more"
        ></a
      >
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 24, 2019
  
</span>

  </div>
</article>

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/cuckoo-filter/">
    <h2 class="post-title">Cuckoo Filter</h2>
  </a>
  <div class="post-entry">
    
      <p>比Bloom-filter更好的过滤器 我们经常会遇到一个需求，就是判断一个元素是否在某个集合里面，首先想到的是维护一个HashMap（比如python中的字典），但是存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就变得很可观了，还需要存储对象的key（防止碰撞后没法确定元素），内存可能会装不下，我们需要一个空间效率更高的数据结构，这个时候我们一般会想到Bloom-filter。Bloom-filter可以看作k个hash函数+Bitmap，原理是通过k个hash函数将元素映射到Bitmap的k个位置，因为Bitmap只用1个bit就可以存储一个值，所以空间效率非常高，关于Bloom-filter，网络上已经有很多介绍的文章，就不赘述了。对于判断一个数据是否在某个海量数据的集合里面，Bloom-filter有奇效，比如应对缓存穿透、垃圾邮件的判断等
但是Bloom-filter并不是完美的，比如Bloom-filter不支持删除，一旦需要删除一个元素，就只能重建Bloom-filter，简单的把元素hash后的k个位置置0会造成其他元素的误判（误判一个已经存在的元素为不存在，破坏了Bloom-filter的特性），对于此，有不少的Bloom-filter的变种，比如counting Bloom，d-left counting Bloom，大致的原理是把Bitmap位数组的每一位扩展为一个小的计数器。增加了实现复杂度的同时还降低了空间利用率，那有没有更好的替代方案呢？所以由此引入这次的主角：Cuckoo-Filter
Cuckoo Hash 介绍Cuckoo-Filter之前得先说说Cuckoo Hash。Cuckoo Hash其实是一种解决hash冲突的策略。
Cuckoo是布谷鸟的意思，也就是我们常说的杜鹃。“鸠占鹊巢”里面的“鸠”的就是类似杜鹃这种鸟类。这种鸟有一种即狡猾又贪婪的习性，它不肯自己筑巢，而是把蛋下到别的鸟巢里，而且它的幼鸟又会比别的鸟早出生，布谷幼鸟天生有一种残忍的动作，幼鸟会拼命把未出生的其它鸟蛋挤出窝巢，今后以便独享“养父母”的食物。借助生物学上这一典故，cuckoo hashing处理碰撞的方法，就是把原来占用位置的这个元素踢走，不过被踢出去的元素还要比鸟蛋幸运，因为它还有一个备用位置可以安置，如果备用位置上还有人，再把它踢走，如此往复。直到被踢的次数达到一个上限，才确认哈希表已满，并执行rehash操作，下图是一个Cuckoo Hash的例子：
Cuckoo hash会有多个hash函数（一般是两个），把一个元素x映射到多个位置上，这一点和Bloom-filter很像，但是Cuckoo hash只会在多个位置中选择一个空的位置来存储x（或其他信息，取决于实现）。如果都满了，就随机选一个位置“踢走”那个元素，把那个元素踢到它自己的替代位置上去，如果它的替代位置也满了，则继续踢。图中就是要插入一个x，发现hash后的两个位置都被占了，所以随机选择了6的位置踢走了元素a，a找到了自己的备用位置4发现已经被c占了，接着把c踢走，c最后找到了一个空的位置1，整个插入过程完成。 这样会造成一个问题，就是插入的开销会在元素越来越多的时候变得越来越大，元素可能会一直踢来踢去，甚至出现死循环，比如下图（极端情况）
元素x经过2个hash后的位置都被占了，随机选择一个位置12，把e踢走，e的备用位置被a占了，踢走a，a又踢走了b，b又踢走了c，c踢走了d，d的备用位置是12，于是又把刚插入的x给踢走了……陷入了死循环，所以需要一个上限，达到上限只能重建。此时可以衡量Cuckoo Hash的最大负载因子，负载因子 = 已经存入的元素 / 总的bucket。上图中这种可以看作只有一个slot的bucket，增加slot比如图c，可以大大减少冲突后踢走元素的概率，每个bucket对应一个hash值，相同hash值的元素都存在同一个bucket的不同slot中，除非2个bucket的8个slot都满了，否则不会踢元素。一维数组实现的cuckoo hash的负载因子大小和其他hash策略没什么区别，只有50%，但是多路slot的实现，比如图c中，能达到90%以上，可以说效率很高了。
Cuckoo-Filter设计 本文主要参考了酷壳和CMU论文的实现，这篇论文首先提出了Cuckoo-Filter，很有参考价值。 论文中在Cuckoo-Filter的设计上比起基本的Cuckoo Hash有不少的优化。作为过滤器，空间肯定是能省就省，所以在每个slot中不会存原本的元素x，而是存的元素x的fingerprint(指纹，记做f)。由于存的是指纹，没有存x本身或者x的备用位置，或者那么在寻找元素的备用位置的时候怎么知道备用位置呢。作者采用了一种比较巧妙的办法，第一个地址用hash(x)来确定，第二个地址用hash(f)和第一个地址做异或来确定，这样不管当前是哪个地址，直接用当前地址和hash(f)做异或就可以得到另外一个地址。同时，插入的时候优先选择空的位置，而不是踢走其他的元素。
刚开始其实还有一个比较疑惑的点，为什么求第二个地址的时候用的是hash(f)来异或呢，而不是直接异或f得到第二个地址，最后在论文中找到了答案(有些词不好翻译就直接贴原文了)：
In addition, the fingerprint is hashed before it is xor-ed with the index of its current bucket to help distribute the items uniformly in the table. If the alternate location were calculated by “i ⊕ fingerprint” without hashing the fingerprint, the items kicked out from nearby buckets would land close to each other in the table, if the size of the fingerprint is small compared to the table size.</p>
      <a href="https://schecterdamien.github.io/posts/2019/cuckoo-filter/" class="post-read-more"
        ></a
      >
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Apr 17, 2019
  
</span>

  </div>
</article>

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/python-debug/">
    <h2 class="post-title">怎么高效的调试python程序(1)</h2>
  </a>
  <div class="post-entry">
    
      <p>这篇文章是受到18年pycon上张翔大神演讲的启发，当时他的演讲的题目是《我的python进程怎么了》，主要是介绍了python进程调试的一些工具和方法，总结的比较全，当时在现场听完感觉受益匪浅，但是一直没去整理，这篇文章大致也是按照他的ppt的大纲来写的。
我们代码写完后，运行时经常会遇到这样那样的问题，有一些很容易就能发现，因为代码跑到这里就直接就报错了，抛异常了，一般看日志的报错信息就能发现具体是哪里出了问题。但是还有一部分是没法从日志中看出来的：“为什么我的python进程卡住了”，“为什么我的python进程消耗这么多的内存”，“为什么我的python进程消耗了这么多的cpu”，卡住了是因为死锁了还是阻塞在io上了？消耗内存多了是因为存在内存泄漏还是数据结构设计的不合理？这种时候就需要一系列的工具来排查，工欲善其事，必先利其器。
print&amp;log 相信对大部分人来说，print和log是最常用的调试方法了。在感觉有问题的地方，加上print和log来输出一些状态信息和变量，每个python程序员都是这方面的大师。
print&amp;log大法简单粗暴，但往往很有效。缺点是需要提前了解你的代码，知道在哪里print&amp;log，而且需要修改后重启你的进程，调试完还要删除调试的代码。
pdb pdb是python自带的一个库，为python程序提供了一种交互的源代码调试功能，主要特性包括设置断点、单步调试、进入函数调试、查看当前代码、查看栈片段、动态改变变量的值等。pdb可以认为是python下的gdb，两者保持了一样的用法。
有两种不同的方法使用pdb，一种是直接在命令行指定参数启动pdb，比如直接python -m pdb pdb_test.py 可以进入命令行调试模式，接下来可以输入相应的调试命令进行调试，比如：l是查看当前代码，b是设置断点，n是执行下一行，单步调试。输入h可以查看各命令的使用说明。
pdb单步执行太麻烦了，第二种方法是直接在代码中import pdb，直接在代码里需要调试的地方放一个 pdb.set_trace() 设置一个断点，程序运行到这里会暂停并进入pdb调试环境，可以用pdb 变量名查看变量，或者c继续运行。关于pdb更详细的使用方法，可以查看官方文档
但是实际工作中，很多时候我们都是使用flask或者django等框架来开发，特别是django，很多时候需要在django shell里面进行一系列的调试，这个时候pdb就不好启动了，可以使用django-pdb。
除了pdb，还有一个第三方的开源的python调试器Ipdb，具有语法高亮、tab补全，更友好的输出信息等高级功能，ipdb之于pdb，就相当于IPython之于Python，虽然都是实现相同的功能，但是，在易用性方面做了很多的改进。如果使用ide，比如PyCharm，很多都自带了断点设置，变量查看的功能，使用上更加方便友好。
sys sys模块包括了一组非常实用的服务，内含很多函数方法和变量，用来处理Python运行时的配置以及资源，从而可以与当前程序之外的系统环境交互，是python程序用来请求解释器行为的接口。
很长一段时间里面，都不清楚sys模块的重要性，说这是python标准库中最重要的模块之一也不为过了，后来陆续接触到sys模块的一些黑科技般的方法，才逐渐的了解到它的强大，里面很多的方法涉及到的背景和知识都能另起一篇文章了，所以这里只介绍一部分。
sys._getframe([depth]): 返回一个栈帧对象，depth为栈顶部向下的深度，默认为0，返回的是当前的栈帧。
&gt;&gt;&gt; import sys &gt;&gt;&gt; frame = sys._getframe() &gt;&gt;&gt; dir(frame) [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;f_back&#39;, &#39;f_builtins&#39;, &#39;f_code&#39;, &#39;f_globals&#39;, &#39;f_lasti&#39;, &#39;f_lineno&#39;, &#39;f_locals&#39;, &#39;f_trace&#39;] &gt;&gt;&gt; assert frame.f_back == None &gt;&gt;&gt; &gt;&gt;&gt; dir(frame.</p>
      <a href="https://schecterdamien.github.io/posts/2019/python-debug/" class="post-read-more"
        ></a
      >
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Feb 24, 2019
  
</span>

  </div>
</article>

      
        <article class="post-preview">
  <a href="https://schecterdamien.github.io/posts/2019/unicode/">
    <h2 class="post-title">unicode到底是个什么鬼</h2>
  </a>
  <div class="post-entry">
    
  </div>

  <div class="postmeta">
    <span class="meta-post">
  <em class="fa fa-calendar-alt"></em
  >&nbsp;Jan 24, 2019
  
</span>

  </div>
</article>

      
    </div>
    
      <ul class="pager">
        
        
          <li class="next">
            
              <a href="https://schecterdamien.github.io/page/2/"
                > &rarr;</a
              >
            
          </li>
        
      </ul>
    
  </div>

    <footer>
  

<div class="social-icons">
  
    
    
      
      <a href="link%20to%20social%20media" name="name of social media">
        <em class="A icon from https://fontawesome.com/"></em>
      </a>
    
  

  
</div>


  
  <div class="container">
    <p class="credits copyright">
      <a href="https://schecterdamien.github.io/about">wujinjing</a>
      &nbsp;&copy;
      2021
      
      &nbsp;&ndash;&nbsp;
      <em class="fas fa-moon" id="dark-mode-toggle"></em>
    </p>

    <p class="credits theme-by">
       <a href="https://gohugo.io">Hugo</a>&nbsp;
      
      <a href="https://github.com/matsuyoshi30/harbor">Harbor</a>
    </p>
  </div>
</footer>

  </body>
</html>
