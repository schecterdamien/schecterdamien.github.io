<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Firsy</title>
    <link>https://schecterdamien.github.io/</link>
    <description>Recent content on Firsy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch</language>
    <lastBuildDate>Sat, 27 Aug 2022 22:37:38 +0800</lastBuildDate><atom:link href="https://schecterdamien.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>数状数组</title>
      <link>https://schecterdamien.github.io/posts/2022/binary-indexed-tree/</link>
      <pubDate>Sat, 27 Aug 2022 22:37:38 +0800</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2022/binary-indexed-tree/</guid>
      <description>树状数组 树状数组或二元索引树（Binary Indexed Tree），又以其发明者命名为Fenwick树，最早由Peter M. Fenwick于1994年以A New Data Structure for Cumulative Frequency Tables为题发表在SOFTWARE PRACTICE AND EXPERIENCE。
其初衷是解决数据压缩里的累积频率（Cumulative Frequency）的计算问题，现多用于高效计算数列的前缀和， 区间和。它可以以O(log n)的时间得到任意前缀和，并同时支持在O(log n)时间内支持动态单点值的修改。空间复杂度O(n)
怎么理解树状数组要解决的问题呢？ 举例来说，树状数组所能解决的典型问题就是存在一个长度为n的数组，我们如何高效进行如下操作：
add(idx, num)：将num加到位置idx的数字上。 prefixSum(idx)：求从数组第一个位置到第idx（含idx）的所有数字的和。 rangeSum(from_idx, to_idx)：求从数组第from_idx个位置到第to_idx个位置的所有数字的和 要解决以上问题，可以有以下几种思路
最开始想到的，肯定就是直接基于原数组去暴力实现这几个函数，这样，add就是O(1)时间复杂度，predixSum就是O(n)时间复杂度，rangeSum(from_idx，to_idx)也是O(n)时间复杂度，但是一般这种是不符合要求的，本来就是要解决特定场景下的问题，查询的时间复杂度太高 或者再建立一个前缀和数组，每一位保存前面所有数的累加值，额外的空间复杂度是O（1），prefixSum(idx)时间复杂度是O(1)，rangeSum(from_idx，to_idx)=prefixSum(to_idx)-prefixSum(from_idx)时间复杂度为O(1)，但是add(idx，num)就需要更新idx到数组末尾的每一个位的值，时间复杂度O（n），除非add操作非常低频，否则这种做法也不合适 数组数组这种数据结构能平衡求前缀和和更新操作，使之都是O（logn）时间复杂度，虽然需要额外的O（n）空间
怎么做到的 数状数组逻辑上可以看作一颗多叉树，父节点等于子节点之和，但是实际存储还是按照数组的形式来存储数组的。 树状数组的父节点和子节点的下标关系是parent = son + 2^k，其中 k 是子节点下标对应二进制末尾 0 的个数，也就是说一个节点的子节点是符合以下条件的节点：子节点的下标加上下标二进制形式最末尾的1所代表的数等于父节点的下标。
比如8的二进制是 0b1000，那么有 7（0b111）加上最末尾的1正好等于8、6（0b110）加上最末尾的1也就是2（0b10）正好等于8、4（0b100）加上最末尾的1也就是4（0b100）正好等于8&amp;hellip;&amp;hellip;
再观察上图，可以发现一些规律
树状数组里奇数节点正好等于原数组对应节点的值，因为奇数在最末尾肯定是1，1后面没有0。有2个子节点的，正好其二进制末尾的1后面有1个0。有3个子节点的，正好末尾后面有2个0。有4个子节点的，其末尾后面有3个0。可以引申一个节点的子节点的个数 n = k + 1，其中 k 是子节点下标对应二进制末尾 0 的个数 树状数组里某个节点的值对应原数组m个节点的值累加，m=2^k，其中 k 是子节点下标对应二进制末尾 0 的个数。且这m个节点是连续的 树状节点的下标为i，那对应到原数组里m个连续节点的左边界是i-2^k+1，右边界是i。其中 k 是子节点下标对应二进制末尾 0 的个数 可以发现树状节点很重要的一个操作是要求序号二进制最后一位1，也就是lowbit，可以定义一个函数：
func lowBit(x int) int { return x &amp;amp; -x } 这里用到了位运算的技巧，因为数值在计算机中都是用补码表示的，负数等于正数原码按位取反再+1，再和原数进行 &amp;amp; 操作就可以得到lowbit，随便找个数换成二进制形式算一下就能理解了。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://schecterdamien.github.io/about/</link>
      <pubDate>Tue, 23 Aug 2022 22:50:36 +0800</pubDate>
      
      <guid>https://schecterdamien.github.io/about/</guid>
      <description>联系方式：schecterfirst@126.com</description>
    </item>
    
    <item>
      <title>HTTP/2 抓包遇到的坑</title>
      <link>https://schecterdamien.github.io/posts/2021/http2-wireshark/</link>
      <pubDate>Thu, 23 Dec 2021 22:35:18 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2021/http2-wireshark/</guid>
      <description>上一篇文章介绍了http2协议相关的细节。因为是为了准备分享，所以为了尽可能直观的展示协议通信（特别是握手协商）的整个过程，还是决定准备一下小的demo然后抓包演示下。在这个过程也遇到了一些问题，耽搁了不少时间，所以记录一下整个过程
h2c抓包 分别对HTTP2的两种建立链接的方式进行抓包（h2c和h2），先演示h2c的连接建立过程，准备的server端的代码如下：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;log&amp;#34; &amp;#34;net/http&amp;#34; &amp;#34;golang.org/x/net/http2&amp;#34; &amp;#34;golang.org/x/net/http2/h2c&amp;#34; ) // http2 h2c func main() { h2s := &amp;amp;http2.Server{} handler := http.NewServeMux() handler.HandleFunc(&amp;#34;/ping&amp;#34;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprint(w, &amp;#34;pong&amp;#34;) }) s := &amp;amp;http.Server{ Addr: &amp;#34;:5005&amp;#34;, Handler: h2c.NewHandler(handler, h2s), } http2.ConfigureServer(s, &amp;amp;http2.Server{}) log.Fatal(s.ListenAndServe()) } 然后打开wireshark，监听本地的5005端口，使用curl请求server，这里需要指定&amp;ndash;http2使用http2协议，不然的话curl默认使用http1.1：
curl --http2 http://localhost:5005/ping 可以在wireshark里看到建立连接的过程：
上图是客户端(curl)发送的http1.1的协议升级请求。这里能看到上篇文章提到的协议升级的关键的header。Connection指定header有哪些逐跳头部，Upgrade指定客户端希望升级到http2协议，HTTP2-Settings则指定了连接的初始参数。
上图是服务端的http response。返回了Upgrade表示服务端同意升级到http2协议，然后客户端和服务端就可以使用http2通信了。
h2c抓包遇到的问题 虽然在上文已经提到了结论：
go在1.6开始支持HTTP/2，这个时候只支持HTTP/2 over TLS，也就是h2。只要是TLS部署，则http库就会默认进行HTTPS/2协商，协商失败则蜕化为HTTPS/1 go在1.8开始支持 HTTP/2 server push go在1.11开始支持 HTTP/2 h2c gin目前只支持h2方式的HTTP/2，不支持h2c方式的HTTP/2 但是在一开始对于go里是怎么支持http2是不太清楚的，是在第三方库里支持还是net/http直接支持？是h2c和h2两种方式都支持还是只支持h2？google的时候很多文章又语焉不详，所以在准备一段h2c server代码的时候比较曲折。</description>
    </item>
    
    <item>
      <title>HTTP/2 协议</title>
      <link>https://schecterdamien.github.io/posts/2021/http2/</link>
      <pubDate>Sun, 25 Jul 2021 21:27:17 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2021/http2/</guid>
      <description>用grpc很久了，但是一直都仅限于使用，对于比较深层次的细节还是缺乏了解，想找个时间好好学习下这个黑盒底下的一系列技术，最近刚好要准备一个技术分享，所以就决定分享HTTP/2协议相关的细节，因为grpc的通信就是用的HTTP/2协议，这篇文章算是对准备的HTTP/2分享的一个记录
http应该大家都不陌生，目前使用最多的应该还是HTTP/1.1版本的http协议，那么HTTP/1.1到底有些什么样的问题，导致需要HTTP/2呢？主要有两点
头阻塞：HTTP/1.1中只有收到当前请求响应后才能重用当前tcp连接发送下一次请求。虽然HTTP/1.1提出了pipeline，旨在缓解这个问题。但是一方面pipeline在复杂的网络环境下很难实现和普及，其次就算都用上了pipeline，pipeline也只是使得可以在本次响应完成之前可以发送下次请求，但响应依然要严格按照顺序返回，也就是如果前一个响应被阻塞，后边的响应将不会到来，头阻塞问题还是没有完全解决。目前客户端（比如浏览器）一般会同时打开多个tcp连接来绕开头阻塞的问题，chrome默认会打开6个tcp链接，并发请求各种类型数据。但是这就带来了一个矛盾，tcp链接越多，肯定对资源的浪费是越大的，链接越少，头阻塞的问题就会越突出 重复的未压缩头数据的传输：自HTTP/1.1之后，HTTP请求中通常带有大量ASCII编码的头部，这些头部通常大部分不会变化，需要每次请求都携带（尤其像是User Agent、Cookie这些值比较长的头部字段），会给本来就拥挤的网络带来很大的压力 除此之外还有一些问题，比如TCP 的慢启动，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度，而大量http的业务请求其实本身都只是短链接的，所以导致tcp的流控算法没有很好的应用。为此，HTTP/2的出现就很有必要了
HTTP/2 概述 先说下HTTP/2的发展历程:
2009年，Google提出了一项实验性的协议SPDY（读音同speedy），旨在开发者不修改当前网站实现的前提下，提高页面加载速度。SPDY提出后，Chrome、Firefox、Opera等主流浏览器先后给出了实现，很多大型网站（如Google、Twitter、Facebook等）分别提供了其对SPDY会话的实现 2012年，HTTP-WG提出了在SPDY基础上构建HTTP/2的草案 2013年给出了第一个对HTTP/2的实现，自此HTTP/2、SPDY并行发展，在客户端和服务器上进行了广泛可靠的测试 2015年，Google 宣布放弃对SPDY的继续支持，标志着HTTP/2正式登上历史舞台 那么HTTP/2有些什么特点呢：
HTTP/2 可以让我们的应用更快、更简单、更稳定 通过有效压缩 HTTP 标头字段将协议开销降至最低，同时增加对请求优先级和服务器推送的支持 带来了大量其他协议层面的辅助实现，例如新的流控制、错误处理和升级机制 HTTP/2 没有改动 HTTP 的应用语义，而是修改了数据的格式和传输方式，只需要升级基础设施（代理、web框架等），上层的业务代码都可以不必修改而在新协议下运行 上一个HTTP版本是1.1，为什么这之后不是 HTTP/1.2呢？因为 HTTP/2 引入了一个新的二进制分帧层，该层无法与之前的 HTTP/1.x 服务器和客户端兼容，因此协议的主版本提升到 HTTP/2，且HTTP/2后HTTP协议没有小版本号了，后面直接就是HTTP3
那HTTP/2是怎么做到加入了这么多功能和优化，但是上层业务不需要改代码呢？一切都是因为引入了一个二进制分帧层，并且HTTP/2的新特性也都是建立在这个基础之上的。这又印证了那句话，“计算机中任何问题都可以通过增加一个中间层来解决“。
如上图，HTTP/2.0在应用层（HTTP）和传输层（TCP或者TLS）之间增加一个二进制分帧层，HTTP/2.0通信都在一个TCP连接上完成，这个连接可以承载任意数量的双向数据流，相应的每个数据流以消息的形式发送。而消息由一或多个帧组成，这些帧可以乱序发送，然后根据每个帧首部的流标识符重新组装
这是另一张描述连接（connection）、流（stream）、消息（message）、帧（frame）关系的图，这里解释下这几个概念。
帧（Frame）： 帧是HTTP/2通信的最小单位 请求和响应都分为首部帧和消息帧单独传输，初此之外还有一些其他的类型的帧，下面会介绍帧结构的时候会说 消息（Message）： 指逻辑上的HTTP消息，比如一次请求、一次响应等。 由一个或多个帧组成，以二进制压缩格式存放 HTTP/1 中的头部。 流（Stream）： 是一条逻辑上的连接，是TCP连接中的一个虚拟通道（每个Tcp连接会建立多个steam） 可以承载双向的消息，每个流都有一个唯一的整数标识符，帧会记录Stream 的id 不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ），因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息。同一 Stream 内部的帧必须是严格有序的。 客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。 同一个连接中的 Stream ID 是不能复用的，只能顺序递增，所以当 Stream ID 耗尽时，需要发一个控制帧 GOAWAY，用来关闭 TCP 连接 连接（Connection）： 即TCP连接 这种在一个TCP连接中划分多个逻辑链接，把所有两端的流量都集中在一个TCP连接的做法，减少了服务端的压力，虽然流量还是这么多流量，但是创建的socket会明显减少，内存占用更少，每个连接吞吐量更大。并且HTTP 性能优化的关键并不在于高带宽，而是低延迟，这种做法避免了TCP的慢启动，能更有效的利用到TCP的流控算法。</description>
    </item>
    
    <item>
      <title>mysql锁总结</title>
      <link>https://schecterdamien.github.io/posts/2021/mysql-lock/</link>
      <pubDate>Thu, 04 Mar 2021 23:35:01 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2021/mysql-lock/</guid>
      <description>最近做了一次和MySQL（innoDB）锁有关的技术分享，记录一下
为了能尽量准确简洁的描述innoDB的锁机制，看了好多的文章，由于innoDB中的锁系统确实非常复杂，细节特别多，如有纰漏和谬误，还请联系改正
innoDB锁简介 innoDb支持多种粒度的锁，按照粒度来分，可分为表锁（LOCK_TABLE）和行锁（LOCK_REC） 一般的锁系统都会有共享锁和排他锁的分类，共享锁也叫读锁，排他锁也叫写锁。加在同一个资源上，写锁会阻塞另外一把写锁或读锁的获取，读锁则允许另外一把读锁的获取，也就是读读之间允许并发，读写或者写写会阻塞，innodb中表锁和行锁都支持共享锁（简写S）和排他锁（简写X）。
因为innoDB支持多粒度的锁，允许表锁和行锁的并存，为了方便多粒度锁冲突的判断，innoDB中还存在一种名叫意向锁（Intention Locks）的锁。
除此之外，还有一种特殊的表锁，自增锁，主要用来并发安全的生成自增id，一种特殊的意向锁，插入意向锁，用来防止幻读问题
表锁 表锁，锁定的粒度是整个表，也分共享锁和排他锁。不同于行锁，表锁MySQL Server层就有实现（所以MyISAM支持表锁，也只支持表锁），innoDb则在存储引擎层面也实现了一遍表锁（后面会介绍具体结构）。
哪些时候会触发表锁呢？在执行某些ddl时，比如alter table等操作，会对整个表加锁，也可以手动执行锁表语句：LOCK TALBES table_name [READ | WRITE]，READ为共享锁，WRITE为排他锁，手动解锁的语句为：UNLOCK TABLES，会直接释放当前会话持有的所有表锁
有一些需要注意的地方：
因为MySQL Server层和InnoDB都实现了表锁，仅当autocommit=0、innodb_table_lock=1（默认设置）时，InnoDB层才能知道MySQL加的表锁，ＭySQL Server才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁，否则，InnoDB将无法自动检测并处理这种死锁 在用LOCK TALBES显式获取锁后，只能访问加锁的这些表，并且如果加的是共享锁，那么只能执行查询操作，而不能执行更新操作，如果获得的是排他锁，则可以进行更新操作。 开始事务时会自动UNLOCK之前的表锁，COMMIT或ROLLBACK都不能释放用LOCK TABLES加的表级锁。LOCK TALBES时会先隐式提交事务，再锁表，UNLOCK TALBES也会隐式提交事务。所以，事务中需要的表锁必须在事务开头一次性获取，无法再事务中间获取，因为不管是LOCK TALBES还是UNLOCK TALBES都会提交事务 官网上建议的表锁的使用方法：
SET autocommit=0; LOCK TABLES t1 WRITE, t2 READ, ...; ... do something with tables t1 and t2 here ... COMMIT; UNLOCK TABLES; 实际业务中，没有特殊理由，不建议使用表锁，因为锁的粒度太大了，极大的影响并发
意向锁 意向锁是一种特殊的表级锁，意向锁是为了让InnoDB多粒度的锁能共存而设计的。取得行的共享锁和排他锁之前需要先取得表的意向共享锁（IS）和意向排他锁（IX），意向共享锁和意向排他锁都是系统自动添加和自动释放的，整个过程无需人工干预。 主要是用来辅助表级和行级锁的冲突判断，因为Innodb支持行级锁，如果没有意向锁，则判断表锁和行锁冲突的时候需要遍历表上所有行锁，有了意向锁，则只要判断表是否存在意向锁就可以知道是否有行锁了。表级别锁的兼容性如下表：
X IX S IS X Conflict Conflict Conflict Conflict IX Conflict Compatible Conflict Compatible S Conflict Conflict Compatible Compatible IS Conflict Compatible Compatible Compatible 可以看到，意向锁与意向锁兼容，IX、IS自身以及相互都兼容，不互斥，因为意向锁仅表示下一层级加什么类型的锁，不代表当前层加什么类型的锁；IX和表级X、S互斥；IS和表级X锁互斥。其兼容性正好体现了其作用</description>
    </item>
    
    <item>
      <title>PostgreSQL的事务隔离和MVCC</title>
      <link>https://schecterdamien.github.io/posts/2019/pg-mvcc/</link>
      <pubDate>Thu, 25 Apr 2019 00:36:43 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2019/pg-mvcc/</guid>
      <description>因为之前遇到的并发问题，所以又把pg的事务隔离和mvcc实现温习了一遍，稍微整理，遂有此文。
数据库并发问题 说到事务隔离，得先说说数据库可能产生的几种并发问题：
脏读（Dirty Read）：一个事务a读到了事务b更新未提交的数据，b回滚了，a读到的数据就是脏数据 不可重复读（NonRepeatable Read）：事务a多次读取同一个数据，事务b在a多次读取的过程中，对这条数据做出了更改并提交，导致事务a在多次读取结果不一致 幻读（Phantom Read）：事务a在批量更新一个表，事务b这个时候插入了一条不同的记录，导致事务a在改完后发现还有一条记录没有改，就像幻觉一样 不可重复读和幻读很容易混淆，其实幻读是不可重复读的一种特殊情况，只不过不可重复读侧重于修改，幻读侧重于新增或删除，解决不可重复读的问题只需要锁住满足条件的行，解决幻读需要提高事务的隔离级别，但与此同时，事务的隔离级别越高，并发能力也就越低。所以，所以还需要权衡。
事务隔离级别 为了有效保证并发读取数据的正确性，提出的事务隔离级别：
未提交读（Read Uncommitted）：允许脏读，也就是可能读取到其他事务中未提交的修改 提交读(Read Committed)：只能读取到其他事务已经提交的数据。PostgreSQL、Oracle等多数数据库默认都是该级别 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 在SQL标准中，定义了不同隔离级别会出现的并发问题：
隔离级别 脏读 不可重复读 幻读 未提交读 可能 可能 可能 已提交读 不可能 可能 可能 可重复读 不可能 不可能 可能 串行读 不可能 不可能 不可能 注意，这是ANSI SQL标准中的定义，但是具体到数据库的实现可能不一样，只会更严格。比如Read Uncommitted这种隔离级别，其实在实际使用中是没有意义的，设置这种级别还不如用Nosql，所以在pg 中其实Read Uncommitted和Read Committed是一样的。
关于Repeated Read，因为在同一个事务内查询到的数据都是在开启事务后第一次查询时候的快照，可知在业务层开启事务后，事务内所有的乐观锁机制都将失效，毕竟都读不到其他事务的提交了，一定会造成写冲突。除此之外，看了不少的讲pg的事务隔离的文章，都是说pg的Repeated Read不会出现幻读&amp;hellip;然后实际上，自测发现并不能完全避免。因为幻读其实分为好多种，在《A Critique of ANSI SQL Isolation Levels》论文就定义了好多种的幻读。可以说，pg的Repeated Read基于其MVCC的实现可以避免一部分，但是是无法完全避免的。 关于Serializable，pg在9.1之前，是没有Repeated Read这个隔离级别，只有Serializable。9.1之后，才把之前的Serializable重命名为Repeated Read，然后加上一个更为严格的Serializable隔离级别。在这两个隔离级别下，如果两个不同事务中同时修改一条记录都会导致其中某个事物写失败，所以在应用层需要有重试机制。 关于两者的不同，应该是说Serializable这种隔离级别可以解决更多的幻读问题，Serializable使用谓词锁（Predicate Lock）来防止这些幻读，意思是如果一个事务T1正在执行一个查询，该查询的的WHERE子句存在一个条件表达式E1，同时这个查询下面还有其他更新或者插入操作，那么另外一个事务T2 就不能插入或删除任何满足E1的数据行。比如之前遇到的并发问题，其实可以简单表达为(伪代码)：
if not Binding.objects.filter(sku_code=&amp;#39;test&amp;#39;).all(): # 这里只有用all()才能触发pg的谓词锁，first()，exist()都不行 Binding.objects.create(sku_code=&amp;#39;test&amp;#39;) 如果是在Repeated Read下，可能就重复创建两条sku_code=&amp;lsquo;test&amp;rsquo;的Binding了，但是在Serializable里面因为存在读写依赖，并发两条事务中肯定会又一条会失败，会提示&amp;rsquo;could not serialize access due to read/write dependencies among transactions&amp;rsquo;错误</description>
    </item>
    
    <item>
      <title>记一次并发问题的排查</title>
      <link>https://schecterdamien.github.io/posts/2019/concurrency-problem/</link>
      <pubDate>Wed, 24 Apr 2019 21:31:41 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2019/concurrency-problem/</guid>
      <description>起源 事情的起源是这样的，Binding是一张多对多的表，主要有sku_code，settlement_sn，enable这几个字段，但是逻辑上sku_code，settlemen_sn，enable=True的在表中应该是唯一的。因为enable=False可能有多条，所以不能在数据库上加联合唯一的索引，但是代码里面有判断。然后有一天测试发现出现两条记录的sku_code，settlemen_sn相同且enable都为True，然后看代码，创建Binding的代码大致是这样的
def create_binding(sku_code, settlement_sn)： if not Binding.objects.filter(sku_code=sku_code, settlement_sn=settlement_sn, enable=True).exist(): raise OperationError(&amp;#39;binding_created&amp;#39;) # 一系列有些耗时的校验 do_some_check() return Binding.objects.create(settlement_sn=settlement_sn, sku_code=sku_code, enable=True) 先判断这样的Binding是否存在，存在的话就直接抛异常，不存在的话再进行其他的校验，最后生成对应的Binding。一般情况下没问题，但是如果并发请求的sku_code和settlement_sn都相同，接到请求的几个worker同时从数据库查询发现目标binding不存在，做出可以创建的判断（因为下面存在一些耗时的校验让这种情况的概率变得很高），然后就创建了sku_code和settlement_sn都相同的记录。 因为是公司内部使用的财务规则系统，也就几个人在用，所以做的时候没怎么考虑并发问题。最后发现其实是前端没有做按钮的防重点，用户连续在按钮上点了两下，造成了这个问题。 我们用的是pg（等下会说为什么强调是pg），这里其实是发生了幻读。那改事务隔离级别可以解决吗，可重复读和串行化不是能解决幻读吗。。。。。肯定不行，一言不合就改数据库事务隔离级别肯定是不行的，而且改了也没法解决这种幻读问题(后来才知道其实串行化可以，但需要加上失败重试的逻辑)。那加锁吧！当时也没多想，迷迷糊糊的在判断上加个锁：
def create_binding(sku_code, settlement_sn)： if not Binding.objects.filter(sku_code=sku_code, settlement_sn=settlement_sn, enable=True).select_for_update).exist(): raise OperationError(&amp;#39;binding_created&amp;#39;) ..... 查询上加上select_for_update()，阻塞住并发的读，完美！发到测试环境，一看，并不行&amp;hellip;还是会生成重，再回头看才明白这个锁有问题。首先，django的select_for_update加上exist()其实是不会生成&amp;rsquo;select &amp;hellip; for update&amp;rsquo;语句的，然后就算生成了，但是因为没有这样的数据，所以没有row可以被锁，这个锁其实没有任何作用。
解决 那怎么解决呢，分布式锁肯定是可以的，给判断的代码块加上锁，这样创建binding的请求就变成串行的了，如果对并发没要求，比如我们这种场景为了防止重点，可以使用这种办法。代码块就变成这样了：
def create_binding(sku_code, settlement_sn)： # 获取分布式锁 get_lock() if not Binding.objects.filter(sku_code=sku_code, settlement_sn=settlement_sn, enable=True).exist(): raise OperationError(&amp;#39;binding_created&amp;#39;) # 一系列有些耗时的校验 do_some_check() result = Binding.objects.create(settlement_sn=settlement_sn, sku_code=sku_code, enable=True) # 释放分布式锁 release_lock() return result 除此之外有没有别的办法呢，其实我们就是想锁住一个“不存在”的一行，google了一下，搜到一篇文章：https://rosscoded.com/blog/2018/05/02/locking-phantom-postgresql/，这里面在pg的数据库级别给出了解决办法。首先可以使用ON CONFLICT语句，其次还可以使用advisory locks，劝告锁，这是pg独有的一种锁，思路挺有意思，不过这是库级别的锁，用多了也不好。但是这些都得在代码里面写sql，最后我们还是选择了基于redis的简单的分布式锁来解决。
回到之前说的，为啥强调是pg呢，因为后来试了一下，在mysql里面，其实是可以给不存在的数据加锁的，mysql里面这个叫gap lock，间隙锁，专门用来解决幻读问题。比如直接select * from binding where sku_code = &amp;quot;123&amp;quot; for update其实会同时加record Lock和gap lock，把已经存在的sku_code=123的行给锁住，同时还会锁住不存在的这些目标行，就是这时其他事务其实是没法写sku_code=123的数据的。mysql只有在可重复读以上的隔离级别才会自动加gap lock。</description>
    </item>
    
    <item>
      <title>Cuckoo Filter</title>
      <link>https://schecterdamien.github.io/posts/2019/cuckoo-filter/</link>
      <pubDate>Wed, 17 Apr 2019 23:34:49 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2019/cuckoo-filter/</guid>
      <description>比Bloom-filter更好的过滤器 我们经常会遇到一个需求，就是判断一个元素是否在某个集合里面，首先想到的是维护一个HashMap（比如python中的字典），但是存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就变得很可观了，还需要存储对象的key（防止碰撞后没法确定元素），内存可能会装不下，我们需要一个空间效率更高的数据结构，这个时候我们一般会想到Bloom-filter。Bloom-filter可以看作k个hash函数+Bitmap，原理是通过k个hash函数将元素映射到Bitmap的k个位置，因为Bitmap只用1个bit就可以存储一个值，所以空间效率非常高，关于Bloom-filter，网络上已经有很多介绍的文章，就不赘述了。对于判断一个数据是否在某个海量数据的集合里面，Bloom-filter有奇效，比如应对缓存穿透、垃圾邮件的判断等
但是Bloom-filter并不是完美的，比如Bloom-filter不支持删除，一旦需要删除一个元素，就只能重建Bloom-filter，简单的把元素hash后的k个位置置0会造成其他元素的误判（误判一个已经存在的元素为不存在，破坏了Bloom-filter的特性），对于此，有不少的Bloom-filter的变种，比如counting Bloom，d-left counting Bloom，大致的原理是把Bitmap位数组的每一位扩展为一个小的计数器。增加了实现复杂度的同时还降低了空间利用率，那有没有更好的替代方案呢？所以由此引入这次的主角：Cuckoo-Filter
Cuckoo Hash 介绍Cuckoo-Filter之前得先说说Cuckoo Hash。Cuckoo Hash其实是一种解决hash冲突的策略。
Cuckoo是布谷鸟的意思，也就是我们常说的杜鹃。“鸠占鹊巢”里面的“鸠”的就是类似杜鹃这种鸟类。这种鸟有一种即狡猾又贪婪的习性，它不肯自己筑巢，而是把蛋下到别的鸟巢里，而且它的幼鸟又会比别的鸟早出生，布谷幼鸟天生有一种残忍的动作，幼鸟会拼命把未出生的其它鸟蛋挤出窝巢，今后以便独享“养父母”的食物。借助生物学上这一典故，cuckoo hashing处理碰撞的方法，就是把原来占用位置的这个元素踢走，不过被踢出去的元素还要比鸟蛋幸运，因为它还有一个备用位置可以安置，如果备用位置上还有人，再把它踢走，如此往复。直到被踢的次数达到一个上限，才确认哈希表已满，并执行rehash操作，下图是一个Cuckoo Hash的例子：
Cuckoo hash会有多个hash函数（一般是两个），把一个元素x映射到多个位置上，这一点和Bloom-filter很像，但是Cuckoo hash只会在多个位置中选择一个空的位置来存储x（或其他信息，取决于实现）。如果都满了，就随机选一个位置“踢走”那个元素，把那个元素踢到它自己的替代位置上去，如果它的替代位置也满了，则继续踢。图中就是要插入一个x，发现hash后的两个位置都被占了，所以随机选择了6的位置踢走了元素a，a找到了自己的备用位置4发现已经被c占了，接着把c踢走，c最后找到了一个空的位置1，整个插入过程完成。 这样会造成一个问题，就是插入的开销会在元素越来越多的时候变得越来越大，元素可能会一直踢来踢去，甚至出现死循环，比如下图（极端情况）
元素x经过2个hash后的位置都被占了，随机选择一个位置12，把e踢走，e的备用位置被a占了，踢走a，a又踢走了b，b又踢走了c，c踢走了d，d的备用位置是12，于是又把刚插入的x给踢走了……陷入了死循环，所以需要一个上限，达到上限只能重建。此时可以衡量Cuckoo Hash的最大负载因子，负载因子 = 已经存入的元素 / 总的bucket。上图中这种可以看作只有一个slot的bucket，增加slot比如图c，可以大大减少冲突后踢走元素的概率，每个bucket对应一个hash值，相同hash值的元素都存在同一个bucket的不同slot中，除非2个bucket的8个slot都满了，否则不会踢元素。一维数组实现的cuckoo hash的负载因子大小和其他hash策略没什么区别，只有50%，但是多路slot的实现，比如图c中，能达到90%以上，可以说效率很高了。
Cuckoo-Filter设计 本文主要参考了酷壳和CMU论文的实现，这篇论文首先提出了Cuckoo-Filter，很有参考价值。 论文中在Cuckoo-Filter的设计上比起基本的Cuckoo Hash有不少的优化。作为过滤器，空间肯定是能省就省，所以在每个slot中不会存原本的元素x，而是存的元素x的fingerprint(指纹，记做f)。由于存的是指纹，没有存x本身或者x的备用位置，或者那么在寻找元素的备用位置的时候怎么知道备用位置呢。作者采用了一种比较巧妙的办法，第一个地址用hash(x)来确定，第二个地址用hash(f)和第一个地址做异或来确定，这样不管当前是哪个地址，直接用当前地址和hash(f)做异或就可以得到另外一个地址。同时，插入的时候优先选择空的位置，而不是踢走其他的元素。
刚开始其实还有一个比较疑惑的点，为什么求第二个地址的时候用的是hash(f)来异或呢，而不是直接异或f得到第二个地址，最后在论文中找到了答案(有些词不好翻译就直接贴原文了)：
In addition, the fingerprint is hashed before it is xor-ed with the index of its current bucket to help distribute the items uniformly in the table. If the alternate location were calculated by “i ⊕ fingerprint” without hashing the fingerprint, the items kicked out from nearby buckets would land close to each other in the table, if the size of the fingerprint is small compared to the table size.</description>
    </item>
    
    <item>
      <title>怎么高效的调试python程序(1)</title>
      <link>https://schecterdamien.github.io/posts/2019/python-debug/</link>
      <pubDate>Sun, 24 Feb 2019 11:47:07 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2019/python-debug/</guid>
      <description>这篇文章是受到18年pycon上张翔大神演讲的启发，当时他的演讲的题目是《我的python进程怎么了》，主要是介绍了python进程调试的一些工具和方法，总结的比较全，当时在现场听完感觉受益匪浅，但是一直没去整理，这篇文章大致也是按照他的ppt的大纲来写的。
我们代码写完后，运行时经常会遇到这样那样的问题，有一些很容易就能发现，因为代码跑到这里就直接就报错了，抛异常了，一般看日志的报错信息就能发现具体是哪里出了问题。但是还有一部分是没法从日志中看出来的：“为什么我的python进程卡住了”，“为什么我的python进程消耗这么多的内存”，“为什么我的python进程消耗了这么多的cpu”，卡住了是因为死锁了还是阻塞在io上了？消耗内存多了是因为存在内存泄漏还是数据结构设计的不合理？这种时候就需要一系列的工具来排查，工欲善其事，必先利其器。
print&amp;amp;log 相信对大部分人来说，print和log是最常用的调试方法了。在感觉有问题的地方，加上print和log来输出一些状态信息和变量，每个python程序员都是这方面的大师。
print&amp;amp;log大法简单粗暴，但往往很有效。缺点是需要提前了解你的代码，知道在哪里print&amp;amp;log，而且需要修改后重启你的进程，调试完还要删除调试的代码。
pdb pdb是python自带的一个库，为python程序提供了一种交互的源代码调试功能，主要特性包括设置断点、单步调试、进入函数调试、查看当前代码、查看栈片段、动态改变变量的值等。pdb可以认为是python下的gdb，两者保持了一样的用法。
有两种不同的方法使用pdb，一种是直接在命令行指定参数启动pdb，比如直接python -m pdb pdb_test.py 可以进入命令行调试模式，接下来可以输入相应的调试命令进行调试，比如：l是查看当前代码，b是设置断点，n是执行下一行，单步调试。输入h可以查看各命令的使用说明。
pdb单步执行太麻烦了，第二种方法是直接在代码中import pdb，直接在代码里需要调试的地方放一个 pdb.set_trace() 设置一个断点，程序运行到这里会暂停并进入pdb调试环境，可以用pdb 变量名查看变量，或者c继续运行。关于pdb更详细的使用方法，可以查看官方文档
但是实际工作中，很多时候我们都是使用flask或者django等框架来开发，特别是django，很多时候需要在django shell里面进行一系列的调试，这个时候pdb就不好启动了，可以使用django-pdb。
除了pdb，还有一个第三方的开源的python调试器Ipdb，具有语法高亮、tab补全，更友好的输出信息等高级功能，ipdb之于pdb，就相当于IPython之于Python，虽然都是实现相同的功能，但是，在易用性方面做了很多的改进。如果使用ide，比如PyCharm，很多都自带了断点设置，变量查看的功能，使用上更加方便友好。
sys sys模块包括了一组非常实用的服务，内含很多函数方法和变量，用来处理Python运行时的配置以及资源，从而可以与当前程序之外的系统环境交互，是python程序用来请求解释器行为的接口。
很长一段时间里面，都不清楚sys模块的重要性，说这是python标准库中最重要的模块之一也不为过了，后来陆续接触到sys模块的一些黑科技般的方法，才逐渐的了解到它的强大，里面很多的方法涉及到的背景和知识都能另起一篇文章了，所以这里只介绍一部分。
sys._getframe([depth]): 返回一个栈帧对象，depth为栈顶部向下的深度，默认为0，返回的是当前的栈帧。
&amp;gt;&amp;gt;&amp;gt; import sys &amp;gt;&amp;gt;&amp;gt; frame = sys._getframe() &amp;gt;&amp;gt;&amp;gt; dir(frame) [&amp;#39;__class__&amp;#39;, &amp;#39;__delattr__&amp;#39;, &amp;#39;__dir__&amp;#39;, &amp;#39;__doc__&amp;#39;, &amp;#39;__eq__&amp;#39;, &amp;#39;__format__&amp;#39;, &amp;#39;__ge__&amp;#39;, &amp;#39;__getattribute__&amp;#39;, &amp;#39;__gt__&amp;#39;, &amp;#39;__hash__&amp;#39;, &amp;#39;__init__&amp;#39;, &amp;#39;__init_subclass__&amp;#39;, &amp;#39;__le__&amp;#39;, &amp;#39;__lt__&amp;#39;, &amp;#39;__ne__&amp;#39;, &amp;#39;__new__&amp;#39;, &amp;#39;__reduce__&amp;#39;, &amp;#39;__reduce_ex__&amp;#39;, &amp;#39;__repr__&amp;#39;, &amp;#39;__setattr__&amp;#39;, &amp;#39;__sizeof__&amp;#39;, &amp;#39;__str__&amp;#39;, &amp;#39;__subclasshook__&amp;#39;, &amp;#39;clear&amp;#39;, &amp;#39;f_back&amp;#39;, &amp;#39;f_builtins&amp;#39;, &amp;#39;f_code&amp;#39;, &amp;#39;f_globals&amp;#39;, &amp;#39;f_lasti&amp;#39;, &amp;#39;f_lineno&amp;#39;, &amp;#39;f_locals&amp;#39;, &amp;#39;f_trace&amp;#39;] &amp;gt;&amp;gt;&amp;gt; assert frame.f_back == None &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; dir(frame.</description>
    </item>
    
    <item>
      <title>unicode到底是个什么鬼</title>
      <link>https://schecterdamien.github.io/posts/2019/unicode/</link>
      <pubDate>Thu, 24 Jan 2019 16:01:06 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2019/unicode/</guid>
      <description> 编码问题可以说是python中非常让人头疼的一个问题了，看到知乎上一个哥们说：一旦你走上了编程之路，如果你不把编码问题搞清楚，那么它就像幽灵一般纠缠你整个职业生涯，各种灵异事件会接踵而来。只有充分发挥程序员死磕到底的精神你才有可能彻底摆脱编码问题带来的烦恼。 实在不能更同意了。前几天看到一篇文章写的挺不错https://zhuanlan.zhihu.com/p/40834093。网上也有很多类似的文章，一般都会说到python3的str的编码是unicode。同时在另一本知名的书上《effective python》有这样一段：
把Unicode字符表示为二进制数据（也就是原始8位值）有许多的办法。最常见的编码方式就是UTF-8。但是大家要记住，python3的str实例和python2的unicode实例都没有和特定的二进制编码形式相关联。要把Unicode字符转换成二进制数据，就必须使用encode方法。想要把二进制数据转换成Unicode字符，则必须使用decode方法。
看到这里我就很分裂了，那python3的str到底是什么编码格式呢？如果没有和特定的二进制编码形式相关联，那str在内存中怎么表示呢？unicode到底是个什么鬼？
Unicode 说到这里，就得去研究下unicode。
刚开始没有的unicode的时候，英语国家使用ascii，中国使用gbk，日本使用shift_jis……大家都使用自己的编码协议，大家交流起来就特别麻烦，一不小心就乱码了。假设没有一门可以包容所有语言的编码，同时有n个适用于各国自己语言的编码，那大家需要交流的话就要n*(n-1)种映射协议来转换编码，让大家都能交流，这无疑对互联网来说是场灾难。所以unicode出现了，它对世界上大部分的文字系统进行了整理、编码，使得计算机可以用更为简单的方式来呈现和处理文字。
大概来说，Unicode编码系统可分为编码方式和实现方式两个层次。重点来了，Unicode的编码方式和实现方式是不同的，编码方式是指的字符集，也就是指定了每个值和字符的映射关系，比如我可以自己制定一个很简单的字符集类似于:{1: &#39;你&#39;, 2: &#39;好&#39;，……，260: &#39;呢&#39;}，这里指定的是值和字母的映射关系，1代表的就是&amp;rsquo;你&amp;rsquo;，2代表的是&amp;rsquo;好&amp;rsquo;，260代表的是&amp;rsquo;呢&amp;rsquo;。根据编码方式我们可以有不同的实现方式。比如如果是用定长编码来实现，要存储260这个中文字符，我们至少需要两个字节，所以我们可以写死用两个字节来存储，尽管对于前面256个字符来说，第一个字节都是0。如果使用变长编码来实现，可以把前面128个汉字用一个字节表示，留出首位当作标识位，其他的汉字用两个字节表示。还有比如两个字节是大端序还是小端序会影响到第一个字节是高位还是第二个字节是高位等等，这些都是不同的实现方式。
所以Unicode我们通常说的是指的其编码方式，就是映射关系。Unicode的实现方式称为Unicode转换格式（Unicode Transformation Format，简称为UTF）。我们用的比较多的有UTF-8，UTF-16，UTF-32等。所以我们说python3中的str的编码方式是unicode这个说法就有问题了，unicode只是一个字符集，没有约定实现，具体的实现还要看是采用UTF-8还是UTF-16还是别的方式，那到底python中采取的是unicode的哪种实现方式呢？
真相大白 然而这个问题，在中文网络上几乎没找到相关的答案，看了好多的博客，都只是说python3的str编码是unicode，这句话本身就有问题。最后还是在stackoverflow上找到了答案: What is internal representation of string in Python 3.x 以及 How is unicode represented internally in Python?。
总结一下，就是python在3.3之前，包括python2，具体的内部unicode采用哪种方式实现其实是编译时的选项。取决于系统的本机字节顺序以及是否选择了UCS2或UCS4。UCS4的话就是采用UTF-32来实现，UCS2的话就采用UTF-16来实现，本机字节顺序决定了是UTF-16 BE还是UTF-16 LE。也就是一旦在编译的时候设置好了，之后python里面unicode的实现都是这种方式。python3.3之后，采用了一种更为灵活的方式来表示，字符串可能是ascii, latin-1, utf-8, utf-16, utf-32等编码形式中的一种，就是说不再是写死一种方式了（这个地方还没完全弄明白，先留个坑），这个主要是pep 393中约定的：PEP 393 &amp;ndash; Flexible String Representation
参考链接 https://www.cnblogs.com/malecrab/p/5300503.html https://zh.wikipedia.org/wiki/Unicode https://zh.wikipedia.org/wiki/通用字符集#Unicode和ISO_10646的关系 </description>
    </item>
    
    <item>
      <title>namedtuple第一个参数有什么用？</title>
      <link>https://schecterdamien.github.io/posts/2018/namedtuple/</link>
      <pubDate>Wed, 19 Dec 2018 19:36:33 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2018/namedtuple/</guid>
      <description>namedtuple是python中一个简单实用的数据结构，但是很不爽的是每次使用的时候都需要给第一个参数指定一个字符串，而这个字符串在实际使用中似乎并没有什么作用。比如
&amp;gt;&amp;gt;&amp;gt; from collections import namedtuple &amp;gt;&amp;gt;&amp;gt; Student = namedtuple(&amp;#39;StudentTuple&amp;#39;, [&amp;#39;sex&amp;#39;, &amp;#39;name&amp;#39;, &amp;#39;age&amp;#39;]) &amp;gt;&amp;gt;&amp;gt; s = Student(&amp;#39;male&amp;#39;, &amp;#39;callmedad&amp;#39;, 22) &amp;gt;&amp;gt;&amp;gt; s.age 22 我操作的对象都是Student，而不是StudentTuple，那我为什么还要传一个&amp;rsquo;StudentTuple&amp;rsquo;，这个字符串似乎对我没有任何帮助。stackoverflow同样有人提了这个问题：传送门1，传送门2. 看了下回答，然后又看了下namedtuple的源码，总算明白了。 接着上面的代码，我们可以看下Student是个啥。
&amp;gt;&amp;gt;&amp;gt; import inspect &amp;gt;&amp;gt;&amp;gt; inspect.isclass(Student) True &amp;gt;&amp;gt;&amp;gt; Student &amp;lt;class &amp;#39;__main__.StudentTuple&amp;#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; Student &amp;lt;class &amp;#39;__main__.StudentTuple&amp;#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; s.__class__ &amp;lt;class &amp;#39;__main__.StudentTuple&amp;#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; type(namedtuple) &amp;lt;class &amp;#39;function&amp;#39;&amp;gt; 我们大致知道这几点：1. Student是个类，并且只是一个类的引用，查看s.__class__知道，这个类定义的名字是StudentTuple，就是我们传的第一个参数。2. namedtuple是个方法，但是返回的是一个新的类，也就是说这是一个类的工厂方法。
既然是一个生成类的工厂方法，那么肯定就需要知道类的名字，所以这就是第一个参数的意义。然后，这一切是怎么实现的呢？
看下namedtuple的源码，其实非常简单粗暴
def namedtuple(typename, field_names, *, verbose=False, rename=False, module=None): ... class_definition = _class_template.format( typename = typename, # 第一个参数 field_names = tuple(field_names), # 各种检验后的属性列表 num_fields = len(field_names), arg_list = repr(tuple(field_names)).</description>
    </item>
    
    <item>
      <title>关于语言的杂想</title>
      <link>https://schecterdamien.github.io/posts/2018/about-absurd/</link>
      <pubDate>Mon, 19 Nov 2018 00:52:56 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2018/about-absurd/</guid>
      <description>最近在看西西弗神话，有一些感想。
首先，其实没看多少，但是真的是看的很费劲，很头疼。因为，实在是看不明白，这个看不明白我感觉或许不是因为我理解不了，或许是更深层次的无力感。加缪所描述的荒谬感我觉得我应该再能感同身受不过了，我也是通过一些其他途径发现荒谬感这一说法，然后顺着源头找到了加缪。但是结果很沮丧，因为翻译的实在太生硬了，主谓宾都不全，单从词的翻译看来或许还好，但是一旦汇成一个句子，看起来还真的很“荒谬”。或许对于小说来说，翻译错一点不要紧，某种程度上来说小说的“容错率“比较高，因为信息密度不高，但是对一部讲哲学的散文来说，差之毫厘，谬以千里。
从另一个角度来看，翻译确实是很难，不同语言之间的表达其实不是一对一的映射。不是说其他一个语言的词映射成中文就是一定是某个词，要是这样，那翻译器的代码我都能写。很多情况是多对多的，比如同样是表示秋天，英文有fall，有autumn，而fall还有&amp;quot;落&amp;quot;的意思，除了映射秋这个季节本身，还额外附带“萧条“、”伤感“、”衰落”之类的主观情感体验，可能上下文语境稍微有点差异，译文的词就有不同的选择，所以翻译更要求，你要想翻译，首先你要完全读懂，明白作者想表达什么，然后翻译。但这是最理想的情况，翻译不可能不带主观感受，你看到的本来就已经是一个二手的理解了，可能区别不大，也可能面目全非，所以有条件有能力的话看书最好还是看原版&amp;hellip;
上面是从翻译的角度说，从写作的人的角度来说，也不能说没有责任。我发现文学家写的哲学感悟往往和哲学家写的哲学感悟是不一样的。这是感性和理性的区别。感性放佛天生就带有美学的有色眼镜，而作者可能是最为感性的群体之一，而且美学往往一点都不直白，很隐晦，很“曲折”，很多层的转义。所以作家写一个东西的时候，他不会很直白的去写，而是很扭扭捏捏去描写。比如西西弗神话里面有一句很著名的话，“荒诞感，在随便哪条街上，都会直扑随便哪个人的脸上”。初一听，这个表达好牛逼，但是仔细想，应该是想表达荒诞感每个人都会有的对吧？但是我也不敢很肯定是这个意思。于是，无形中增加了我们理解的成本，甚至误解的概率，当然，这种表达是很“美“的，是文学家所追求的。但是哲学家是很“吝啬”的，他们往往单刀直入，直白的让你怀疑这个东西我所理解的真实是他们想表达的吗，所以我感觉哲学家是很理性的，很有逻辑性的。
更深层次的 —— 我们可以说都是“巴别塔“惹的祸。圣经有个广为流传的神话，就是古巴比伦人想建造一个能通天的巴别塔，由于大家语言相通，同心协力，高塔直插云端，彷佛没多久真的可以通天了，上帝看到了，就很愤怒，降下灾罚，让人们的语言不再统一，人与人之间难以直接交流，于是最终塔就半途而废了。结合上面说的那些问题，不都是语言的问题吗？语言作为信息的一个载体，两个人要交流，需要一个人写或者说，这个是把自己所想用语言表达出来，这个是表达能力，是编码器，另一个人需要把文字翻译成内心的感受，这个是理解能力，是解码器。编码器和解码器都不是完美的，所以信息就肯定会有失真。所以才会有那么多争吵，很多时候大家表达的东西同一个意思，只不过我编码有问题，或者你解码有问题。同时这也是为什么同一个文化环境下的人之间互相好理解些，因为大家的编码和解码的算法差不多啊，所以误解的概率就小。这还是同一种语言，不同的语言之间还要加了一层“翻译“，那更是痛苦。不得不说，上帝的这个惩罚真是狠，从此没有人能完全理解另一个人，每个人都在信息的孤岛上自说自话&amp;hellip;
关于语言我隐约还有另一个感觉，现在还不是特别明显，但是已经有了一点迹象。就是我感觉很多时候很多哲学或者终极问题本身，其实就是陷入了语言理解的死循环中。佛法中的一个句式“佛说某某，即非某某，是名某某”，名指的是语言背后的抽象概念，诸子百家就有一个“名家”是专门研究这个的，这个“名”很关键，代表语言所表达的本质。越来越感觉语言是限制我们更深刻理解世界的阻碍。语言是个很高效的理解事物的办法，利于知识的传播，但是表达的准确性是很不够的，以致于我们越来越深陷于语言表面所描绘的描绘的事物。佛学对这一方面的研究很深，对语言很警惕，很多看起来很玄乎的佛学对话其实就是一个意思：我不能说，因为我说的佛法和你理解的佛法肯定不是一个东西，甚至当我说出“佛法”的时候，就错了，佛法本身的定义可能我和你都有差别。“凡所有相,皆是虚妄,若见诸相非相,即见如来”。 然后我又想到了禅宗里面讲究顿悟，刚开始我始终理解不了顿悟是什么感觉，有人说，顿悟的感觉就是：有人一直和你说和牛很好吃，特别好吃，你知道了，你也觉得和牛好吃，但是你真的知道吗？直到有一天你真的吃到了和牛，你才感觉到，真他妈好吃！之前我根本就不知道原来是这种味道！大概顿悟就是这种感觉吧。这或许还是语言的锅，好吃有千万种，我无法通过一个“好吃”，准确表达我的感受&amp;hellip;&amp;hellip;这是永远做不到的事。
这样越思考就越悲观了，有时候会幻想在圣经的世界里面，上帝发怒之前，人们是怎么一种状态呢，我更愿意相信他们之间连语言都没有&amp;hellip;..或者有一种能百分百交流的语言，那真的就是世界大同了，真这样还有人类做不成的事情？难怪上帝会害怕。 我相信在很遥远的未来，人类会以另一种形态存在，不在以“语言”这种低效的沟通方式交流，或许那时候，各种“孤独感”、“虚无感”、”荒谬感“就会消失吧。</description>
    </item>
    
    <item>
      <title>判断多个区间是否有重叠</title>
      <link>https://schecterdamien.github.io/posts/2018/interval-tree/</link>
      <pubDate>Wed, 10 Oct 2018 18:26:25 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2018/interval-tree/</guid>
      <description>今天遇到个需求，大致意思是前端给我一堆商品按照阶梯定价的规则，比如订单数为0-200，价格是10块，订单数200-300，价格是8块……简化的数据结构是这样的：
[ { &amp;#34;min_orders&amp;#34;: 0, &amp;#34;max_orders&amp;#34;: 200, &amp;#34;price&amp;#34;: 10 }, { &amp;#34;min_orders&amp;#34;: 200, &amp;#34;max_orders&amp;#34;: 300, &amp;#34;price&amp;#34;: 8 }, { &amp;#34;min_orders&amp;#34;: 400, &amp;#34;max_orders&amp;#34;: 500, &amp;#34;price&amp;#34;: 7 } ] 我需要对前端传的数据做校验，其中有一部分就是不同阶的上下界不能有交叉（前端给的阶梯数据不一定有序），比如有0-200的阶梯，如果再有100-300的阶梯那这个数据就有问题，因为没法定价。抽象出来问题其实就是判断多个区间（可能不连续）是否有重叠。
初一看，这个问题其实很简单，两个for循环，第一层记住遍历过的区间，第二层把当前区间和之前遍历的区间一个个比较看是否交叉就可以了。这么做当然没问题，但是时间复杂度是O(n^2)，怎么看都觉得应该有更好的解决办法，第一遍的遍历是怎么都省不了的，所以优化到极致也不可能比O(n)低，第二遍其实就是个比较的过程，既然是比较的话，那就有思路了，可以联想到很多类似的问题，本质上和排序问题是差不多的。
在stackoverflow看到一个相同的问题：search for interval overlap in list of intervals?。里面提到了一个办法：可以先对区间以左端点进行排序，然后再遍历区间，比较当前区间的左端点和上一个区间的右端点，如果当前的左端点比上一个右端点小，那么一定就有重叠的部分。第一遍排序时间复杂度是O(nlogn)，第二遍遍历时间复杂度是O(n)，所以总体的还是O(nlogn)，比如：
[(4,6), (7,9), (1,5), (6,7)] 排序完之后是：
[(1,5), (4,6), (6,7), (7,9)] 然后进行遍历比较，遍历到第二个区间的时候，左端点是4，比上个区间的右端点5要小，所以肯定有重叠的部分。
第二种办法是构建区间树，上面说的效率最低的那种解决办法，第二层for循环是比较，既然是比较，那么就可以通过树这种结构来提高比较的效率，而不用和前面所有区间进行比较，所以很自然就想到可以构建一个二叉排序树，树的叶节点是每个区间。同时，树的查询效率和树的深度是有关系的，所以构建一个相对平衡的二叉排序树也能进一步提高效率，比如红黑树。算法导论上介绍了这种数据结构，节点是区间的红黑树——区间树。
红黑树实现起来还是很麻烦的，找了一圈发现github上有现成的轮子——intervaltree，使用非常简单：
&amp;gt;&amp;gt;&amp;gt; from intervaltree import Interval, IntervalTree &amp;gt;&amp;gt;&amp;gt; tree = IntervalTree() &amp;gt;&amp;gt;&amp;gt; iv = Interval(4, 7) &amp;gt;&amp;gt;&amp;gt; tree.add(iv) &amp;gt;&amp;gt;&amp;gt; target_iv = Interval(5, 8) &amp;gt;&amp;gt;&amp;gt; tree.</description>
    </item>
    
    <item>
      <title>我的第一篇博客</title>
      <link>https://schecterdamien.github.io/posts/2018/first-post/</link>
      <pubDate>Wed, 19 Sep 2018 17:25:19 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/posts/2018/first-post/</guid>
      <description>折腾博客时的一些感想 折腾了好久，博客终于搭出个大概，有个能看的样子，本来用github+hexo就可以很简单搞定的问题，为什么会浪费这么多时间呢，因为一刚开始想的是自己独立建站，完全自己维护。
首先，得有个服务器吧，还要有个域名吧。要存文章，tag，评论什么的肯定还要数据库吧，postgresql这么先进，就他吧。后端就用熟悉的flask吧，貌似最近还出了个flask-rest，可以自动生成restful风格的接口，嗯，拿来用用学习下，源码好像不多，要不趁机都看看。部署需要的工具链gunicorn、supervisor、nginx这几个肯定是要吧。对了，还有前端，正好学学前端比较火的框架，就vue吧，主要是比react简单一点，现在不都流行前后端完全分离吗，再小的项目也要有个好的架构啊，不然以后拓展就麻烦了！那就再加个node做中间层转发。好像差不多，对了还要做好安全方面的措施，还有容灾备份啊，而且博客最重要就是页面的设计，这个得好好参考下，是不是要买本设计的书看看…………
结果，最后就是拖了几个月，就买了服务器、域名、设计了表，其他的一直无限期拖延。仔细想想，貌似这是我一直以来的毛病，分析这个过程，又引出了更多的感想。
为什么要写博客 为什么要写博客，因为很多知识需要沉淀下来，需要“持久化”，人的大脑是快，就像内存，但是容量有限，很多知识不及时记下来的话，就会丢失，那其实学了和没学一样，对自身技术的进步不利。另一方面，想法也是一样，想法不沉淀下来，一闪而过，其实也相当于没出现，写下来也有助于思考，写出来的东西永远都是思考过一遍的，自我加工的。更重要的是，如果不经常写，那这种写的能力就会退化，到了最后内心澎湃万千，下笔或出口都平平淡淡，甚至会错位。那，其实对我来说，重要是要写东西，而不是使劲折腾怎么搭建博客，当然博客漂亮对我来说也重要，别人看着舒服，我自己写着也开心，但是现在在这上面浪费了太多的时间，以至最终什么都没写，实在是本末倒置。
方法论 做事情，方法论很重要，我需要的是尽快搭建起一个博客可以写东西，但是上面的方案里面，vue以及页面设计我完全不熟，在这上面卡的时间是最多的。仔细想，我其实是在搭博客的过程中掺杂了很多其他的期望，比如能借此学习到很多新的技术，但是没有仔细思考可能付出的成本，在构想方案的时候我并没有意识到这点，于是无形中提高了门槛，最终又没有一个得到适当的正反馈，导致一再拖延（缺少正反馈是所有拖延的主要原因），所以这个做事情的方法论是有问题的，我应该是先用自己熟悉的技术，自己掌握的住的来快速实现一版，迅速看到效果，得到一个正反馈，然后再反复迭代优化。而且搭建博客和学习新技术其实是两件事，但是我却当成了一件事来做。当然，如果能hold住，那两者可以兼得，这也源于自己没有正确的评估成本。
坚持 一直觉得时间的力量是很强大的，能让渺小变得伟大。一件很小的事情能够坚持做很久（比如几十年如一日的每天坚持跑步）那比在短时间内完成让人惊讶的事情（比如骑行西藏、突击熬夜复习得满分）难度更大，更显的伟大。其实这不是我第一个博客，之前还有个博客，最后死于没有坚持。我知道这很难，所以我不打算定个计划说每周一定要更新几篇，也许我会几周不写，也可能感觉来了一天写几篇，我并没有赋予博客其他的意义和目的性，也不想达成一个每天都坚持写博客的伟大成就，不过放在更广的时间维度上来看，这事我一直在坚持做，写的也是我想写的，最后也没法放弃，或许会让我觉得自己很厉害吧，哈哈。</description>
    </item>
    
    <item>
      <title>Archive page</title>
      <link>https://schecterdamien.github.io/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/archives/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Search</title>
      <link>https://schecterdamien.github.io/search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://schecterdamien.github.io/search/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
